{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T17:42:28.737685Z",
     "start_time": "2024-07-06T17:42:27.491355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ],
   "id": "cd7736665fb4275f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Airline Flight Delay Analysis and Prediction\n",
    "\n",
    "## Collected Datasets\n",
    "\n",
    "The core data used for this analysis comes from the [Airline Delay Analysis](https://www.kaggle.com/datasets/sherrytp/airline-delay-analysis?resource=download) dataset available on Kaggle. Data hase been selected for 2019, with the primary CSV containing over 10 million rows of data. Due to the size of this dataset, it has been reduced down via sampling.\n",
    "\n",
    "To fetch the full dataset, go to the [provided link](https://www.kaggle.com/datasets/sherrytp/airline-delay-analysis?resource=download), download the '2019' csv file, then unzip and place the file in the `data` directory of this project. Then run the following code blocks to select a sample size, and create/save a new sample.\n"
   ],
   "id": "a98f7e378756b563"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T22:08:21.324037Z",
     "start_time": "2024-07-05T22:08:21.276615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_slider = widgets.IntSlider(value=100000, min=10000, max=1000000, step=10000, description='Sample Size:')\n",
    "display(sample_slider)"
   ],
   "id": "c1a267e9f3df2b6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IntSlider(value=100000, description='Sample Size:', max=1000000, min=10000, step=10000)"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "84ae9af020a64b72ae819d3c0dabb62c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T23:11:55.554699Z",
     "start_time": "2024-07-05T23:11:47.848087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if os.path.exists('data/2019.csv'):\n",
    "    df = pd.read_csv('data/2019.csv')\n",
    "    df = df.sample(n=sample_slider.value, random_state=42)\n",
    "    df.to_csv('data/2019_sample.csv', index=False)"
   ],
   "id": "e9a3ab2e61a87efc",
   "outputs": [],
   "execution_count": 155
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Additional data collected includes a list of all United States airport codes mapped to their respective latitude and longitude coordinates. This data is available [on GitHub](https://raw.githubusercontent.com/ip2location/ip2location-iata-icao/master/iata-icao.csv). ",
   "id": "bd74197d1696ebf2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T17:43:49.542666Z",
     "start_time": "2024-07-06T17:43:49.233718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not os.path.exists('data/airports.csv'):\n",
    "    airports_df = pd.read_csv('https://raw.githubusercontent.com/ip2location/ip2location-iata-icao/master/iata-icao.csv')\n",
    "    airports_df.to_csv('data/airports.csv', index=False)"
   ],
   "id": "efc5ce40d5ed92ab",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Parsing and Cleaning Data\n",
    "\n",
    "The collected and sampled data contains many columns that are unnecessary or redundant for this analysis. These columns will be removed.\n",
    "- OP_CARRIER_FL_NUM - The flight number is arbitrarily chosen by the airline and has no bearing on the chance of flight delay.\n",
    "- DEP_DELAY - This will usually be directly correlated with ARR_DELAY, but ARR_DELAY is more indicative of actual airline performance. For example, some airlines will fly faster and burn more fuel to \"make up time\" enroute.\n",
    "- TAXI_OUT, WHEELS_OFF, WHEELS_ON, TAXI_IN - These just represent the amount of time it takes to taxi the aircraft to and from the gate and runway. This directly correlates either with the delay value itself or with the airport (as some airports have longer taxi times due to runway configurations and ground congestion). Airlines will have compensated for this in their flight time planning.\n",
    "- AIR_TIME - Another measure of time, indicating how long the aircraft was in the air. By the time the aircraft is in the air, the root cause of the delays will have already occurred, rendering a predictive analysis no longer useful.\n",
    "- CARRIER_DELAY, NAS_DELAY, SECURITY_DELAY, LAT_AIRCRAFT_DELAY - These columns in the dataset attempt to separate out delay minutes by root cause. These columns could prove useful for a detailed analysis of flight delay causes, however these values are not known at the time a predictive analysis would be useful."
   ],
   "id": "b299d20ddf222eba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T23:53:55.461934Z",
     "start_time": "2024-07-05T23:53:55.265493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('data/2019_sample.csv')\n",
    "df = df.drop(columns = ['OP_CARRIER_FL_NUM', 'DEP_DELAY'])\n",
    "df = df.drop(columns = ['TAXI_OUT', 'WHEELS_OFF', 'WHEELS_ON', 'TAXI_IN', 'ARR_TIME', 'AIR_TIME'])\n",
    "df = df.drop(columns = ['CARRIER_DELAY', 'NAS_DELAY', 'WEATHER_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY'])\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "df = df.dropna(subset=['FL_DATE', 'DEP_TIME', 'ARR_DELAY'])"
   ],
   "id": "26db656ee37dab9f",
   "outputs": [],
   "execution_count": 165
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The remaining columns will be transformed using custom preprocessing steps in a pipeline. This allows the input of the data to be in an intuitive format before transformations are applied.",
   "id": "7c3ea3b784e4c862"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "### Date/Time Transformer\n",
    "\n",
    "This custom transformer takes in the provided data and performs the following transformations:\n",
    "- Transform `FL_DATE` into `FL_DAY`, converting a string date from \"yyyy-mm-dd\" format to the number of days since January 1st\n",
    "- Transform `DEP_TIME` into `DEP_TIME_MINUTES`, converting a number formatted as \"hhmm\" into the number of minutes since midnight\n",
    "- Transform `FL_DATE` into `DAY_OF_WEEK`, converting the date into the day of the week 0-6 Mon-Sun\n",
    "\n",
    "Note: This is also defined in `tranformers.py` for easy import into other project notebooks."
   ],
   "id": "8dbfa5c6536feca5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T23:02:06.096420Z",
     "start_time": "2024-07-05T23:02:06.084803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DateTimeTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        X_transformed['FL_DAY'] = X['FL_DATE'].apply(self.days_since_year_start)\n",
    "        X_transformed['DEP_MINUTES'] = X['DEP_TIME'].apply(self.minutes_since_midnight)\n",
    "        X_transformed['DAY_OF_WEEK'] = X['FL_DATE'].apply(self.day_of_week)\n",
    "        return X_transformed.drop(columns = ['FL_DATE', 'DEP_TIME'])\n",
    "\n",
    "    def days_since_year_start(self, date_str: str) -> int:\n",
    "        input_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "        january_first = datetime(input_date.year, 1, 1)\n",
    "        days_difference = (input_date - january_first).days\n",
    "        return days_difference\n",
    "\n",
    "    def minutes_since_midnight(self, time_float: float) -> int:\n",
    "        hours = int(time_float // 100)\n",
    "        minutes = int(time_float % 100)\n",
    "        total_minutes = hours * 60 + minutes\n",
    "        return total_minutes\n",
    "    \n",
    "    def day_of_week(self, date_str: str) -> int:\n",
    "        input_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "        return input_date.weekday()"
   ],
   "id": "ee7731e677aeb5e6",
   "outputs": [],
   "execution_count": 150
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Airport Latitude/Longitude Transformer\n",
    "\n",
    "This transformer reads the `ORIGIN` and `DEST` columns, each of which are represented by IATA airport codes (i.e. ATL for Atlanta, LAX for Los Angeles), and and creates new latitude and longitude columns `ORIGIN_LAT`, `ORIGIN_LON`, `DEST_LAT`, and `DEST_LON`. \n",
    "\n",
    "Note: This is also defined in `tranformers.py` for easy import into other project notebooks."
   ],
   "id": "bb98105a599832ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T22:08:30.619037Z",
     "start_time": "2024-07-05T22:08:30.614697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AirportLatLongTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        airports_df = pd.read_csv('data/airports.csv')\n",
    "        self.airports_dict = airports_df[['iata', 'latitude', 'longitude']].set_index('iata').T.to_dict('list')\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        for col in ['ORIGIN', 'DEST']:\n",
    "            lat_col = col + '_LAT'\n",
    "            lon_col = col + '_LON'\n",
    "            X_transformed[lat_col] = X[col].apply(lambda x: self.airports_dict[x][0])\n",
    "            X_transformed[lon_col] = X[col].apply(lambda x: self.airports_dict[x][1])\n",
    "        return X_transformed\n"
   ],
   "id": "b4d4295cfeb5d713",
   "outputs": [],
   "execution_count": 144
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Preparing Arrival Delay Values\n",
    "\n",
    "To use classification for delay identification, the arrival delay will be sorted into one of four categories based on the severity of the delay. Arrival delays under 15 minutes are considered on time, delays from 15 minutes to 45 minutes are considered minor, delays from 45 minutes to 2 hours are considered major, and delays beyond two hours are considered severe.\n"
   ],
   "id": "a1adca93b1dacd8d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T23:54:00.790150Z",
     "start_time": "2024-07-05T23:54:00.669372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# df = df['ARR_DELAY'].apply(lambda x: max(x, 0))\n",
    "\n",
    "def categorize_delay(delay):\n",
    "    if delay < 15:\n",
    "        return 'NO_DELAY'\n",
    "    elif delay < 45:\n",
    "        return 'MINOR_DELAY'\n",
    "    elif delay < 120:\n",
    "        return 'MAJOR_DELAY'\n",
    "    else:\n",
    "        return 'SEVERE_DELAY'\n",
    "\n",
    "# Transform into categories for making categorical predictions\n",
    "df['DELAY_CATEGORY'] = df['ARR_DELAY'].apply(categorize_delay)\n",
    "df = df.drop(columns = 'ARR_DELAY')\n"
   ],
   "id": "89cd2c688b9cd778",
   "outputs": [],
   "execution_count": 166
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Persisting Prepared Data\n",
    "\n",
    "With the necessary transformers in place, the data can be saved and used in other notebooks for descriptive and predictive analysis."
   ],
   "id": "deaaa9c567900fb5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T23:56:20.744276Z",
     "start_time": "2024-07-05T23:56:20.576201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df.to_csv('data/2019_prepared.csv', index=False)\n",
    "print(\"Successfully saved prepared data\")"
   ],
   "id": "6753385cb11f1afa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved prepared data\n"
     ]
    }
   ],
   "execution_count": 169
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6ba0396efae94440"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
